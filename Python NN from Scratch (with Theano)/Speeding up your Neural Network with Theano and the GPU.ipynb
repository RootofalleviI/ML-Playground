{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speeding up your Neural Network  with Theano and the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief intro on Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU\n",
    "\n",
    "A decent explanation on the purpose of using GPU in deep learning: [link](https://www.quora.com/Why-are-GPUs-well-suited-to-deep-learning).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theano\n",
    "\n",
    "[Theano](http://deeplearning.net/software/theano/) is a Python library that lets you define, optimize, and evaluate mathematical expressions, espectially with ones with multi-dimensional arrays (`np.ndarrays`). At it's heart, Theano is a compiler for mathematical expressions in Python, which takes your structures and turn their into very efficient code.\n",
    "\n",
    "It is not a programming language (you write in Python) but you still need to:\n",
    "- declare variables and give their types\n",
    "- build expressions for how to put those variables together\n",
    "- compile expression graphs to functions in order to use them for computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the original implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "\n",
    "train_X, train_y = sklearn.datasets.make_moons(5000, noise=0.40) # Generate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a remark, the default floating-point data type is `float64`, but in order to use your GPU you must use `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theano.config.floatX = 'float32'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now begin the setup. Check out [Neural Network from scratch](https://bitbucket.org/rootofallevii/neural-network-from-scratch/src/master/) if any of these seems unfamiliar to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 1000                   # hidden layer contains 1000 nodes\n",
    "n_examples = len(train_X)     # training set size\n",
    "nn_input_dim = 2              # dimension of input layer\n",
    "nn_output_dim = 2             # dimension of output layer\n",
    "\n",
    "epsilon = np.float32(0.01)    # arbitrary learn rate\n",
    "reg_lambda = np.float32(0.01) # arbitrary regularization strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shared Variables\n",
    "\n",
    "Shared variables behave more like ordinary Python variables. They have an explicit value that is persistent. In contrast, symbolic variables (the \"normal\" variable in Theano) are not given explicit value until one is assigned on the excecution of a compiled Theano function. \n",
    "\n",
    "Symbolic variables can be thought of as representing state for the duration of a single execution. Shared variables on the other hand represent state thta remains in memory for the lifetime of the Python reference.\n",
    "\n",
    "Obviously, since we want to constantly train and update weight and bias, parameters $W_1, W_2, b_1, b_2$ need to be shared variables; the data $X$ and $y$ also need to be persistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = theano.shared(train_X.astype('float32'))            # convert to float32\n",
    "y = theano.shared(np.eye(2)[train_y].astype('float32')) # convert to float32\n",
    "\n",
    "# We also give these variables names\n",
    "W1 = theano.shared(np.random.randn(nn_input_dim, ndim).astype('float32'), name='W1')\n",
    "b1 = theano.shared(np.zeros(ndim).astype('float32'), name='b1')\n",
    "W2 = theano.shared(np.random.randn(ndim, nn_output_dim).astype('float32'), name='W2')\n",
    "b2 = theano.shared(np.zeros(nn_output_dim).astype('float32'), name='b2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Propagation\n",
    "\n",
    "Note that Theano contains a built-in softmax function we can use! `T.nnet` has a range of NN-related functions, including sigmoid, ReLu, and cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = X.dot(W1) + b1\n",
    "a1 = T.tanh(z1)\n",
    "z2 = a1.dot(W2) + b2\n",
    "y_hat = T.nnet.softmax(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "\n",
    "Operations in Theano are typically vector/matrix-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_reg = 1./n_examples * reg_lambda/2 * (T.sum(T.sqr(W1)) + T.sum(T.sqr(W2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "\n",
    "Built-in cross-entropy loss function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = T.nnet.categorical_crossentropy(y_hat, y).mean() + loss_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "\n",
    "Based on the input, the model outputs the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = T.argmax(y_hat, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradients\n",
    "\n",
    "The function `T.grad(f, x)` computes the derivative of `f` with respect to variable `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW2 = T.grad(loss, W2)\n",
    "db2 = T.grad(loss, b2)\n",
    "dW1 = T.grad(loss, W1)\n",
    "db1 = T.grad(loss, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile to functions\n",
    "\n",
    "There is no argument since we always use the same shared variables. You might note there is a delay from Jupyter kernel. This is because internally Theano compiles these into C code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_prop = theano.function([], y_hat)\n",
    "calculate_loss = theano.function([], loss)\n",
    "predict = theano.function([], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient\n",
    "\n",
    "The option `update=(var, expr)` tells the function to update the variable `var` with the value from `expr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_step = theano.function([], updates=((W2, W2-epsilon*dW2),\n",
    "                                            (W1, W1-epsilon*dW1),\n",
    "                                            (b2, b2-epsilon*db2),\n",
    "                                            (b1, b1-epsilon*db1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model and test it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper function\n",
    "\n",
    "The function `var.set_value(val)` function assigns value `val` to shared variable `var`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_passes=20000, print_loss=False):\n",
    "    \n",
    "    \"\"\" Learns parameters for the NN and returns the model.\n",
    "        - nums_passes: number of iterations.\n",
    "        - print_loss: True if printing loss for every 1000 iterations. \"\"\"\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    W1.set_value((np.random.randn(nn_input_dim, ndim) / np.sqrt(nn_input_dim)).astype('float32'))\n",
    "    b1.set_value(np.zeros(ndim).astype('float32'))\n",
    "    W2.set_value((np.random.randn(ndim, nn_output_dim) / np.sqrt(ndim)).astype('float32'))\n",
    "    b2.set_value(np.zeros(nn_output_dim).astype('float32'))\n",
    "    \n",
    "    for i in range(0, num_passes):\n",
    "        gradient_step() # batch gradient descent\n",
    "        if print_loss and i % 1 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" %(i, calculate_loss()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and visualize\n",
    "\n",
    "Time to build the NN and plot the decision boundary. Check out [Neural Network from scratch](https://bitbucket.org/rootofallevii/neural-network-from-scratch/src/master/) for explanation on plotting helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN from scratch \n",
    "def plot_decision_boundary(pred_func):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    h = 0.01 \n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model(print_loss=True)\n",
    "plot_decision_boundary(lambda x: predict(x))\n",
    "plt.title(\"Decision boundary for hidden layer size 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to run this optimized code on an GPU-optimized Amazon EC2 instance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
