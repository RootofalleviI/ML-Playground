{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST: ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Credit**: [TensorFlow and deep learning, without a PhD](https://cloud.google.com/blog/products/gcp/learn-tensorflow-and-deep-learning-without-a-phd) (Part 1~10) <br />\n",
    ">\n",
    "> If you have any questions, please read my notes [**here**](). Otherwise, enjoy :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import mnistdata \n",
    "mnist = mnistdata.read_data_sets(\"data\", one_hot=True, reshape=False)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network with 5 layers\n",
    "#\n",
    "# · · · · · · · · · ·          (input data, flattened pixels)       X [batch, 784]   # 784 = 28*28\n",
    "# \\x/x\\x/x\\x/x\\x/x\\x/ ✞     -- fully connected layer (relu+dropout) W1 [784, 200]      B1[200]\n",
    "#  · · · · · · · · ·                                                Y1 [batch, 200]\n",
    "#   \\x/x\\x/x\\x/x\\x/ ✞       -- fully connected layer (relu+dropout) W2 [200, 100]      B2[100]\n",
    "#    · · · · · · ·                                                  Y2 [batch, 100]\n",
    "#     \\x/x\\x/x\\x/ ✞         -- fully connected layer (relu+dropout) W3 [100, 60]       B3[60]\n",
    "#      · · · · ·                                                    Y3 [batch, 60]\n",
    "#       \\x/x\\x/ ✞           -- fully connected layer (relu+dropout) W4 [60, 30]        B4[30]\n",
    "#        · · ·                                                      Y4 [batch, 30]\n",
    "#         \\x/               -- fully connected layer (softmax)      W5 [30, 10]        B5[10]\n",
    "#          ·                                                        Y5 [batch, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define TensorFlow Variables and Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for training data\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "\n",
    "# Placeholder for correct labels\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Placeholder for learning rate as it will decay over time\n",
    "lr = tf.placeholder(tf.float32)\n",
    "\n",
    "# Placeholder for steps as it contributes to variable learning rate\n",
    "step = tf.placeholder(tf.int32)\n",
    "\n",
    "# Probability of keeping a node during dropout, 0.75 at training time and 1.0 at test time\n",
    "pkeep = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Size of Each Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = 784\n",
    "H1 = 200\n",
    "H2 = 100\n",
    "H3 = 60\n",
    "H4 = 30\n",
    "OUTPUT = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.truncated_normal([INPUT, H1], stddev=0.1))\n",
    "B1 = tf.Variable(tf.ones([H1])/10)\n",
    "W2 = tf.Variable(tf.truncated_normal([H1, H2], stddev=0.1))\n",
    "B2 = tf.Variable(tf.ones([H2])/10)\n",
    "W3 = tf.Variable(tf.truncated_normal([H2, H3], stddev=0.1))\n",
    "B3 = tf.Variable(tf.ones([H3])/10)\n",
    "W4 = tf.Variable(tf.truncated_normal([H3, H4], stddev=0.1))\n",
    "B4 = tf.Variable(tf.ones([H4])/10)\n",
    "W5 = tf.Variable(tf.truncated_normal([H4, OUTPUT], stddev=0.1))\n",
    "B5 = tf.Variable(tf.zeros([OUTPUT]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Layers with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = tf.reshape(X, [-1, 784])\n",
    "\n",
    "Y1 = tf.nn.relu(tf.matmul(XX, W1) + B1)\n",
    "Y1d = tf.nn.dropout(Y1, pkeep)\n",
    "\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1d, W2) + B2)\n",
    "Y2d = tf.nn.dropout(Y2, pkeep)\n",
    "\n",
    "Y3 = tf.nn.relu(tf.matmul(Y2d, W3) + B3)\n",
    "Y3d = tf.nn.dropout(Y3, pkeep)\n",
    "\n",
    "Y4 = tf.nn.relu(tf.matmul(Y3d, W4) + B4)\n",
    "Y4d = tf.nn.dropout(Y4, pkeep)\n",
    "\n",
    "Ylogits = tf.matmul(Y4d, W5) + B5\n",
    "Y = tf.nn.softmax(Ylogits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss Function, Accuracy, Optimizer, and Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=Ylogits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "lr = 0.0001 +  tf.train.exponential_decay(0.003, step, 2000, 1/math.e)\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's Rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ********* epoch 1 ********* test accuracy:0.9813 test loss: 8.780099\n",
      "100: ********* epoch 1 ********* test accuracy:0.9691 test loss: 13.901305\n",
      "200: ********* epoch 1 ********* test accuracy:0.9699 test loss: 12.695408\n",
      "300: ********* epoch 1 ********* test accuracy:0.9691 test loss: 12.054609\n",
      "400: ********* epoch 1 ********* test accuracy:0.9692 test loss: 11.150914\n",
      "500: ********* epoch 1 ********* test accuracy:0.9712 test loss: 11.232132\n",
      "600: ********* epoch 2 ********* test accuracy:0.9743 test loss: 10.223649\n",
      "700: ********* epoch 2 ********* test accuracy:0.9768 test loss: 10.650112\n",
      "800: ********* epoch 2 ********* test accuracy:0.9758 test loss: 9.320734\n",
      "900: ********* epoch 2 ********* test accuracy:0.9743 test loss: 10.853048\n",
      "1000: ********* epoch 2 ********* test accuracy:0.9768 test loss: 10.330263\n",
      "1100: ********* epoch 2 ********* test accuracy:0.975 test loss: 10.038742\n",
      "1200: ********* epoch 3 ********* test accuracy:0.9754 test loss: 9.735613\n",
      "1300: ********* epoch 3 ********* test accuracy:0.9761 test loss: 10.398863\n",
      "1400: ********* epoch 3 ********* test accuracy:0.9766 test loss: 9.541002\n",
      "1500: ********* epoch 3 ********* test accuracy:0.9778 test loss: 9.749133\n",
      "1600: ********* epoch 3 ********* test accuracy:0.9797 test loss: 8.745785\n",
      "1700: ********* epoch 3 ********* test accuracy:0.9775 test loss: 9.135292\n",
      "1800: ********* epoch 4 ********* test accuracy:0.9791 test loss: 8.829445\n",
      "1900: ********* epoch 4 ********* test accuracy:0.98 test loss: 9.442329\n",
      "2000: ********* epoch 4 ********* test accuracy:0.9793 test loss: 9.172912\n",
      "2100: ********* epoch 4 ********* test accuracy:0.9772 test loss: 9.880503\n",
      "2200: ********* epoch 4 ********* test accuracy:0.9804 test loss: 9.1356125\n",
      "2300: ********* epoch 4 ********* test accuracy:0.9811 test loss: 8.832769\n",
      "2400: ********* epoch 5 ********* test accuracy:0.9791 test loss: 8.644498\n",
      "2500: ********* epoch 5 ********* test accuracy:0.9787 test loss: 9.619933\n",
      "2600: ********* epoch 5 ********* test accuracy:0.9808 test loss: 8.556827\n",
      "2700: ********* epoch 5 ********* test accuracy:0.9798 test loss: 9.131561\n",
      "2800: ********* epoch 5 ********* test accuracy:0.9806 test loss: 8.868672\n",
      "2900: ********* epoch 5 ********* test accuracy:0.9815 test loss: 8.557081\n",
      "3000: ********* epoch 6 ********* test accuracy:0.9805 test loss: 8.940675\n",
      "3100: ********* epoch 6 ********* test accuracy:0.9809 test loss: 8.754438\n",
      "3200: ********* epoch 6 ********* test accuracy:0.9801 test loss: 9.09566\n",
      "3300: ********* epoch 6 ********* test accuracy:0.9807 test loss: 8.789435\n",
      "3400: ********* epoch 6 ********* test accuracy:0.9813 test loss: 8.884557\n",
      "3500: ********* epoch 6 ********* test accuracy:0.9814 test loss: 8.84161\n",
      "3600: ********* epoch 7 ********* test accuracy:0.9814 test loss: 8.695151\n",
      "3700: ********* epoch 7 ********* test accuracy:0.9811 test loss: 8.88776\n",
      "3800: ********* epoch 7 ********* test accuracy:0.9821 test loss: 8.803479\n",
      "3900: ********* epoch 7 ********* test accuracy:0.9807 test loss: 8.900419\n",
      "4000: ********* epoch 7 ********* test accuracy:0.9822 test loss: 8.753679\n",
      "4100: ********* epoch 7 ********* test accuracy:0.9818 test loss: 8.858008\n",
      "4200: ********* epoch 8 ********* test accuracy:0.9826 test loss: 8.663632\n",
      "4300: ********* epoch 8 ********* test accuracy:0.9811 test loss: 9.4894085\n",
      "4400: ********* epoch 8 ********* test accuracy:0.9823 test loss: 9.518452\n",
      "4500: ********* epoch 8 ********* test accuracy:0.9821 test loss: 9.826076\n",
      "4600: ********* epoch 8 ********* test accuracy:0.9816 test loss: 9.122189\n",
      "4700: ********* epoch 8 ********* test accuracy:0.9828 test loss: 9.235166\n",
      "4800: ********* epoch 9 ********* test accuracy:0.9813 test loss: 9.166729\n",
      "4900: ********* epoch 9 ********* test accuracy:0.9827 test loss: 9.024249\n",
      "5000: ********* epoch 9 ********* test accuracy:0.9815 test loss: 9.495864\n",
      "5100: ********* epoch 9 ********* test accuracy:0.9817 test loss: 9.444647\n",
      "5200: ********* epoch 9 ********* test accuracy:0.9807 test loss: 9.4851\n",
      "5300: ********* epoch 9 ********* test accuracy:0.9818 test loss: 9.734911\n",
      "5400: ********* epoch 10 ********* test accuracy:0.982 test loss: 9.526428\n",
      "5500: ********* epoch 10 ********* test accuracy:0.9812 test loss: 9.405855\n",
      "5600: ********* epoch 10 ********* test accuracy:0.9818 test loss: 9.354448\n",
      "5700: ********* epoch 10 ********* test accuracy:0.981 test loss: 9.461323\n",
      "5800: ********* epoch 10 ********* test accuracy:0.9816 test loss: 9.304663\n",
      "5900: ********* epoch 10 ********* test accuracy:0.982 test loss: 9.362839\n",
      "6000: ********* epoch 11 ********* test accuracy:0.9815 test loss: 9.078019\n",
      "6100: ********* epoch 11 ********* test accuracy:0.9816 test loss: 9.147112\n",
      "6200: ********* epoch 11 ********* test accuracy:0.9819 test loss: 9.247966\n",
      "6300: ********* epoch 11 ********* test accuracy:0.982 test loss: 9.137831\n",
      "6400: ********* epoch 11 ********* test accuracy:0.9826 test loss: 9.298908\n",
      "6500: ********* epoch 11 ********* test accuracy:0.9825 test loss: 9.250552\n",
      "6600: ********* epoch 12 ********* test accuracy:0.9822 test loss: 9.371222\n",
      "6700: ********* epoch 12 ********* test accuracy:0.9817 test loss: 9.618788\n",
      "6800: ********* epoch 12 ********* test accuracy:0.9821 test loss: 9.577515\n",
      "6900: ********* epoch 12 ********* test accuracy:0.9819 test loss: 9.928033\n",
      "7000: ********* epoch 12 ********* test accuracy:0.9821 test loss: 9.810279\n",
      "7100: ********* epoch 12 ********* test accuracy:0.9819 test loss: 9.738046\n",
      "7200: ********* epoch 13 ********* test accuracy:0.9819 test loss: 9.427448\n",
      "7300: ********* epoch 13 ********* test accuracy:0.982 test loss: 9.472814\n",
      "7400: ********* epoch 13 ********* test accuracy:0.9817 test loss: 9.636328\n",
      "7500: ********* epoch 13 ********* test accuracy:0.9821 test loss: 9.754561\n",
      "7600: ********* epoch 13 ********* test accuracy:0.9827 test loss: 9.604597\n",
      "7700: ********* epoch 13 ********* test accuracy:0.9827 test loss: 9.636023\n",
      "7800: ********* epoch 14 ********* test accuracy:0.9825 test loss: 9.845422\n",
      "7900: ********* epoch 14 ********* test accuracy:0.9818 test loss: 9.884203\n",
      "8000: ********* epoch 14 ********* test accuracy:0.9819 test loss: 9.999607\n",
      "8100: ********* epoch 14 ********* test accuracy:0.9824 test loss: 10.074899\n",
      "8200: ********* epoch 14 ********* test accuracy:0.983 test loss: 9.804348\n",
      "8300: ********* epoch 14 ********* test accuracy:0.9826 test loss: 10.0003\n",
      "8400: ********* epoch 15 ********* test accuracy:0.982 test loss: 9.796335\n",
      "8500: ********* epoch 15 ********* test accuracy:0.9821 test loss: 9.769374\n",
      "8600: ********* epoch 15 ********* test accuracy:0.9825 test loss: 9.661306\n",
      "8700: ********* epoch 15 ********* test accuracy:0.9824 test loss: 9.882823\n",
      "8800: ********* epoch 15 ********* test accuracy:0.9823 test loss: 10.07847\n",
      "8900: ********* epoch 15 ********* test accuracy:0.9825 test loss: 9.944837\n",
      "9000: ********* epoch 16 ********* test accuracy:0.9825 test loss: 9.881905\n",
      "9100: ********* epoch 16 ********* test accuracy:0.9826 test loss: 9.912098\n",
      "9200: ********* epoch 16 ********* test accuracy:0.9818 test loss: 9.94682\n",
      "9300: ********* epoch 16 ********* test accuracy:0.9826 test loss: 9.9724655\n",
      "9400: ********* epoch 16 ********* test accuracy:0.9822 test loss: 10.094796\n",
      "9500: ********* epoch 16 ********* test accuracy:0.9826 test loss: 9.988037\n",
      "9600: ********* epoch 17 ********* test accuracy:0.9823 test loss: 10.101712\n",
      "9700: ********* epoch 17 ********* test accuracy:0.9821 test loss: 9.98069\n",
      "9800: ********* epoch 17 ********* test accuracy:0.9821 test loss: 10.113541\n",
      "9900: ********* epoch 17 ********* test accuracy:0.982 test loss: 10.06201\n",
      "10000: ********* epoch 17 ********* test accuracy:0.9823 test loss: 10.087675\n"
     ]
    }
   ],
   "source": [
    "# What we care about:\n",
    "# - a: accuracy\n",
    "# - c: cross-entropy\n",
    "\n",
    "def training_step(i, update_test_data, update_train_data):\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    if update_train_data:\n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        feed_dict={X: batch_X, Y_: batch_Y, pkeep: 1.0, step: i})\n",
    "    if update_test_data:\n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        feed_dict={X: mnist.test.images, Y_: mnist.test.labels, pkeep: 1.0})\n",
    "        \n",
    "        print(str(i) \n",
    "              + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) \n",
    "              + \" ********* test accuracy:\" + str(a) \n",
    "              + \" test loss: \" + str(c))\n",
    "        \n",
    "    sess.run(train_step, {X: batch_X, Y_: batch_Y, pkeep: 0.75, step: i})\n",
    "\n",
    "for i in range(10000+1):\n",
    "    training_step(i, i % 100 == 0, i % 20 == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: 98% Accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
