{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST: CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Credit**: [TensorFlow and deep learning, without a PhD](https://cloud.google.com/blog/products/gcp/learn-tensorflow-and-deep-learning-without-a-phd) (Part 11~13) <br />\n",
    ">\n",
    "> Part one: [MNIST with Feed-forward Neural Network]() <br />\n",
    "> If you have any questions, please read my notes [**here**](). Otherwise, enjoy :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensorflow version 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import mnistdata \n",
    "mnist = mnistdata.read_data_sets(\"data\", one_hot=True, reshape=False)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network structure for this sample:\n",
    "#\n",
    "# · · · · · · · · · ·      (input data, 1-deep)                 X [batch, 28, 28, 1]\n",
    "# @ @ @ @ @ @ @ @ @ @   -- conv. layer 5x5x1=>4 stride 1        W1 [5, 5, 1, 4]        B1 [4]\n",
    "# ∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶                                           Y1 [batch, 28, 28, 4]\n",
    "#   @ @ @ @ @ @ @ @     -- conv. layer 5x5x4=>8 stride 2        W2 [5, 5, 4, 8]        B2 [8]\n",
    "#   ∶∶∶∶∶∶∶∶∶∶∶∶∶∶∶                                             Y2 [batch, 14, 14, 8]\n",
    "#     @ @ @ @ @ @       -- conv. layer 4x4x8=>12 stride 2       W3 [4, 4, 8, 12]       B3 [12]\n",
    "#     ∶∶∶∶∶∶∶∶∶∶∶                                               Y3 [batch, 7, 7, 12] => reshaped to YY [batch, 7*7*12]\n",
    "#      \\x/x\\x\\x/        -- fully connected layer (relu)         W4 [7*7*12, 200]       B4 [200]\n",
    "#       · · · ·                                                 Y4 [batch, 200]\n",
    "#       \\x/x\\x/         -- fully connected layer (softmax)      W5 [200, 10]           B5 [10]\n",
    "#        · · ·                                                  Y [batch, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define TensorFlow Variables and Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for training data\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "\n",
    "# Placeholder for correct labels\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Placeholder for learning rate as it will decay over time\n",
    "lr = tf.placeholder(tf.float32)\n",
    "\n",
    "# Placeholder for steps as it contributes to variable learning rate\n",
    "step = tf.placeholder(tf.int32)\n",
    "\n",
    "# Probability of keeping a node during dropout, 0.75 at training time and 1.0 at test time\n",
    "pkeep = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Size/Depth of Each Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three conv layers with their channel counts, plus a fully connected layer\n",
    "C1 = 6 # first conv layer output depth\n",
    "C2 = 12 # second conv layer output depth\n",
    "C3 = 24 # third conv layer output depth\n",
    "N = 200 # size of fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the variables what we want to learn \n",
    "W1 = tf.Variable(tf.truncated_normal([6, 6, 1, C1], stddev=0.1))  # 6x6 patch, 1 input channel, K output channels\n",
    "B1 = tf.Variable(tf.constant(0.1, tf.float32, [C1]))\n",
    "W2 = tf.Variable(tf.truncated_normal([5, 5, C1, C2], stddev=0.1))\n",
    "B2 = tf.Variable(tf.constant(0.1, tf.float32, [C2]))\n",
    "W3 = tf.Variable(tf.truncated_normal([4, 4, C2, C3], stddev=0.1))\n",
    "B3 = tf.Variable(tf.constant(0.1, tf.float32, [C3]))\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([7 * 7 * C3, N], stddev=0.1))\n",
    "B4 = tf.Variable(tf.constant(0.1, tf.float32, [N]))\n",
    "W5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))\n",
    "B5 = tf.Variable(tf.constant(0.1, tf.float32, [10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 1  # output is 28x28\n",
    "Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "stride = 2  # output is 14x14\n",
    "Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "stride = 2  # output is 7x7\n",
    "Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n",
    "\n",
    "# Reshape the output from the third conv layer before feeding into the fully connected layer\n",
    "YY = tf.reshape(Y3, shape=[-1, 7 * 7 * C3])\n",
    "\n",
    "Y4 = tf.nn.relu(tf.matmul(YY, W4) + B4)\n",
    "YY4 = tf.nn.dropout(Y4, pkeep) # Adding dropout for better performance\n",
    "Ylogits = tf.matmul(YY4, W5) + B5\n",
    "Y = tf.nn.softmax(Ylogits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss Function, Accuracy, Optimizer, and Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=Ylogits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "lr = 0.0001 +  tf.train.exponential_decay(0.003, step, 2000, 1/math.e)\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's Rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ********* epoch 1 ********* test accuracy:0.102 test loss: 234.20418\n",
      "100: ********* epoch 1 ********* test accuracy:0.9428 test loss: 18.609976\n",
      "200: ********* epoch 1 ********* test accuracy:0.9631 test loss: 11.312636\n",
      "300: ********* epoch 1 ********* test accuracy:0.9774 test loss: 7.8123035\n",
      "400: ********* epoch 1 ********* test accuracy:0.9772 test loss: 7.245\n",
      "500: ********* epoch 1 ********* test accuracy:0.9783 test loss: 6.7397647\n",
      "600: ********* epoch 2 ********* test accuracy:0.9831 test loss: 5.443881\n",
      "700: ********* epoch 2 ********* test accuracy:0.981 test loss: 5.5480223\n",
      "800: ********* epoch 2 ********* test accuracy:0.983 test loss: 5.283968\n",
      "900: ********* epoch 2 ********* test accuracy:0.9849 test loss: 4.8893423\n",
      "1000: ********* epoch 2 ********* test accuracy:0.9868 test loss: 4.399448\n",
      "1100: ********* epoch 2 ********* test accuracy:0.9862 test loss: 4.1706843\n",
      "1200: ********* epoch 3 ********* test accuracy:0.9877 test loss: 3.5469286\n",
      "1300: ********* epoch 3 ********* test accuracy:0.9844 test loss: 4.5618267\n",
      "1400: ********* epoch 3 ********* test accuracy:0.986 test loss: 4.4746437\n",
      "1500: ********* epoch 3 ********* test accuracy:0.9871 test loss: 4.139133\n",
      "1600: ********* epoch 3 ********* test accuracy:0.9868 test loss: 4.008687\n",
      "1700: ********* epoch 3 ********* test accuracy:0.9868 test loss: 4.036545\n",
      "1800: ********* epoch 4 ********* test accuracy:0.9874 test loss: 3.5563953\n",
      "1900: ********* epoch 4 ********* test accuracy:0.9874 test loss: 3.662463\n",
      "2000: ********* epoch 4 ********* test accuracy:0.9852 test loss: 4.850012\n",
      "2100: ********* epoch 4 ********* test accuracy:0.9875 test loss: 3.5457644\n",
      "2200: ********* epoch 4 ********* test accuracy:0.9892 test loss: 3.4710631\n",
      "2300: ********* epoch 4 ********* test accuracy:0.9879 test loss: 3.6804814\n",
      "2400: ********* epoch 5 ********* test accuracy:0.9878 test loss: 3.6163845\n",
      "2500: ********* epoch 5 ********* test accuracy:0.9884 test loss: 3.7741818\n",
      "2600: ********* epoch 5 ********* test accuracy:0.9874 test loss: 4.5417824\n",
      "2700: ********* epoch 5 ********* test accuracy:0.9894 test loss: 3.4407246\n",
      "2800: ********* epoch 5 ********* test accuracy:0.9888 test loss: 3.6466355\n",
      "2900: ********* epoch 5 ********* test accuracy:0.9877 test loss: 3.7694786\n",
      "3000: ********* epoch 6 ********* test accuracy:0.9898 test loss: 3.3253148\n",
      "3100: ********* epoch 6 ********* test accuracy:0.9898 test loss: 3.4750977\n",
      "3200: ********* epoch 6 ********* test accuracy:0.9868 test loss: 4.471589\n",
      "3300: ********* epoch 6 ********* test accuracy:0.9892 test loss: 3.598782\n",
      "3400: ********* epoch 6 ********* test accuracy:0.9898 test loss: 3.548179\n",
      "3500: ********* epoch 6 ********* test accuracy:0.9902 test loss: 3.6356854\n",
      "3600: ********* epoch 7 ********* test accuracy:0.9884 test loss: 3.8226347\n",
      "3700: ********* epoch 7 ********* test accuracy:0.9891 test loss: 3.9122007\n",
      "3800: ********* epoch 7 ********* test accuracy:0.9892 test loss: 4.0822926\n",
      "3900: ********* epoch 7 ********* test accuracy:0.9899 test loss: 3.957926\n",
      "4000: ********* epoch 7 ********* test accuracy:0.9901 test loss: 3.8980718\n",
      "4100: ********* epoch 7 ********* test accuracy:0.9892 test loss: 3.8462825\n",
      "4200: ********* epoch 8 ********* test accuracy:0.9889 test loss: 3.7913778\n",
      "4300: ********* epoch 8 ********* test accuracy:0.9891 test loss: 3.803068\n",
      "4400: ********* epoch 8 ********* test accuracy:0.9891 test loss: 3.989704\n",
      "4500: ********* epoch 8 ********* test accuracy:0.9898 test loss: 3.5851645\n",
      "4600: ********* epoch 8 ********* test accuracy:0.9895 test loss: 3.6621928\n",
      "4700: ********* epoch 8 ********* test accuracy:0.9894 test loss: 3.6606424\n",
      "4800: ********* epoch 9 ********* test accuracy:0.9886 test loss: 3.6994827\n",
      "4900: ********* epoch 9 ********* test accuracy:0.9888 test loss: 4.0029864\n",
      "5000: ********* epoch 9 ********* test accuracy:0.989 test loss: 4.1785307\n",
      "5100: ********* epoch 9 ********* test accuracy:0.9898 test loss: 3.887974\n",
      "5200: ********* epoch 9 ********* test accuracy:0.9896 test loss: 3.824495\n",
      "5300: ********* epoch 9 ********* test accuracy:0.9891 test loss: 4.0596876\n",
      "5400: ********* epoch 10 ********* test accuracy:0.9896 test loss: 3.9859257\n",
      "5500: ********* epoch 10 ********* test accuracy:0.9902 test loss: 3.8515642\n",
      "5600: ********* epoch 10 ********* test accuracy:0.9895 test loss: 4.2842007\n",
      "5700: ********* epoch 10 ********* test accuracy:0.9893 test loss: 4.068314\n",
      "5800: ********* epoch 10 ********* test accuracy:0.9901 test loss: 4.019368\n",
      "5900: ********* epoch 10 ********* test accuracy:0.9896 test loss: 3.9555554\n",
      "6000: ********* epoch 11 ********* test accuracy:0.9892 test loss: 4.153995\n",
      "6100: ********* epoch 11 ********* test accuracy:0.9889 test loss: 4.3751144\n",
      "6200: ********* epoch 11 ********* test accuracy:0.9892 test loss: 4.3449187\n",
      "6300: ********* epoch 11 ********* test accuracy:0.9898 test loss: 4.0965986\n",
      "6400: ********* epoch 11 ********* test accuracy:0.9896 test loss: 4.00992\n",
      "6500: ********* epoch 11 ********* test accuracy:0.9899 test loss: 4.0970016\n",
      "6600: ********* epoch 12 ********* test accuracy:0.99 test loss: 4.0713005\n",
      "6700: ********* epoch 12 ********* test accuracy:0.9894 test loss: 4.144312\n",
      "6800: ********* epoch 12 ********* test accuracy:0.9899 test loss: 4.3074255\n",
      "6900: ********* epoch 12 ********* test accuracy:0.9899 test loss: 4.0995464\n",
      "7000: ********* epoch 12 ********* test accuracy:0.9903 test loss: 3.9529529\n",
      "7100: ********* epoch 12 ********* test accuracy:0.9897 test loss: 4.0343895\n",
      "7200: ********* epoch 13 ********* test accuracy:0.9895 test loss: 4.2393436\n",
      "7300: ********* epoch 13 ********* test accuracy:0.9893 test loss: 4.4617085\n",
      "7400: ********* epoch 13 ********* test accuracy:0.9896 test loss: 4.5568223\n",
      "7500: ********* epoch 13 ********* test accuracy:0.9898 test loss: 4.3215938\n",
      "7600: ********* epoch 13 ********* test accuracy:0.9897 test loss: 4.0977874\n",
      "7700: ********* epoch 13 ********* test accuracy:0.99 test loss: 4.138299\n",
      "7800: ********* epoch 14 ********* test accuracy:0.9901 test loss: 4.2007675\n",
      "7900: ********* epoch 14 ********* test accuracy:0.99 test loss: 4.146843\n",
      "8000: ********* epoch 14 ********* test accuracy:0.9901 test loss: 4.280858\n",
      "8100: ********* epoch 14 ********* test accuracy:0.9903 test loss: 4.082347\n",
      "8200: ********* epoch 14 ********* test accuracy:0.9905 test loss: 4.1232653\n",
      "8300: ********* epoch 14 ********* test accuracy:0.9905 test loss: 4.0764713\n",
      "8400: ********* epoch 15 ********* test accuracy:0.9901 test loss: 4.2545695\n",
      "8500: ********* epoch 15 ********* test accuracy:0.9899 test loss: 4.4404874\n",
      "8600: ********* epoch 15 ********* test accuracy:0.9898 test loss: 4.513464\n",
      "8700: ********* epoch 15 ********* test accuracy:0.9898 test loss: 4.270253\n",
      "8800: ********* epoch 15 ********* test accuracy:0.9903 test loss: 4.2154317\n",
      "8900: ********* epoch 15 ********* test accuracy:0.9899 test loss: 4.250609\n",
      "9000: ********* epoch 16 ********* test accuracy:0.9897 test loss: 4.3047714\n",
      "9100: ********* epoch 16 ********* test accuracy:0.9896 test loss: 4.363311\n",
      "9200: ********* epoch 16 ********* test accuracy:0.9901 test loss: 4.3485513\n",
      "9300: ********* epoch 16 ********* test accuracy:0.9898 test loss: 4.248437\n",
      "9400: ********* epoch 16 ********* test accuracy:0.9901 test loss: 4.2082753\n",
      "9500: ********* epoch 16 ********* test accuracy:0.99 test loss: 4.2730603\n",
      "9600: ********* epoch 17 ********* test accuracy:0.99 test loss: 4.30906\n",
      "9700: ********* epoch 17 ********* test accuracy:0.9901 test loss: 4.4342895\n",
      "9800: ********* epoch 17 ********* test accuracy:0.9895 test loss: 4.506453\n",
      "9900: ********* epoch 17 ********* test accuracy:0.9898 test loss: 4.5250607\n",
      "10000: ********* epoch 17 ********* test accuracy:0.9899 test loss: 4.4396386\n"
     ]
    }
   ],
   "source": [
    "# What we care about:\n",
    "# - a: accuracy\n",
    "# - c: cross-entropy\n",
    "\n",
    "def training_step(i, update_test_data, update_train_data):\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    if update_train_data:\n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        feed_dict={X: batch_X, Y_: batch_Y, pkeep: 1.0, step: i})\n",
    "    if update_test_data:\n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        feed_dict={X: mnist.test.images, Y_: mnist.test.labels, pkeep: 1.0})\n",
    "        \n",
    "        print(str(i) \n",
    "              + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) \n",
    "              + \" ********* test accuracy:\" + str(a) \n",
    "              + \" test loss: \" + str(c))\n",
    "        \n",
    "    sess.run(train_step, {X: batch_X, Y_: batch_Y, pkeep: 0.75, step: i})\n",
    "\n",
    "for i in range(10000+1):\n",
    "    training_step(i, i % 100 == 0, i % 20 == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: 99% Accuracy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
